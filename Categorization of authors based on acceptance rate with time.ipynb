{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.stats import spearmanr, pearsonr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "from sklearn import svm, preprocessing\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "import pickle\n",
    "from scipy.signal import find_peaks_cwt\n",
    "\n",
    "doc_id = []\n",
    "year = []\n",
    "citation_count = []\n",
    "rounds = []\n",
    "accepted = []\n",
    "with open('accepted_papers_all_info_final.txt') as infile:\n",
    "    for line in infile:\n",
    "        a = line.split()\n",
    "        doc_id.append(a[0])\n",
    "        year.append(a[1])\n",
    "        rounds.append(a[2])\n",
    "        citation_count.append(a[3])\n",
    "        accepted.append(1)\n",
    "        \n",
    "with open('rejected_papers_all_info_final.txt') as infile:\n",
    "    for line in infile:\n",
    "        a = line.split()\n",
    "        doc_id.append(a[0])\n",
    "        year.append(a[1])\n",
    "        rounds.append(a[2])\n",
    "        citation_count.append(a[3])\n",
    "        accepted.append(0)\n",
    "        \n",
    "d = {'doc_id' : doc_id,\n",
    "     'citations' : citation_count,\n",
    "     'rounds':rounds,\n",
    "     'year' : year,\n",
    "     'accepted' : accepted}\n",
    "document_details = pd.DataFrame(d)\n",
    "\n",
    "doc =[]\n",
    "auth = []\n",
    "with open('DocIdAuthorId.txt') as infile:\n",
    "    for line in infile:\n",
    "        a = line.split()\n",
    "        doc.append(a[0])\n",
    "        auth.append(a[1])\n",
    "        \n",
    "d = {'doc_id' : doc,\n",
    "     'author_id' : auth}\n",
    "author_doc = pd.DataFrame(d)\n",
    "\n",
    "document_details = pd.merge(document_details , author_doc , on='doc_id' , how = 'inner')\n",
    "document_details.year = document_details.year.astype(int)\n",
    "document_details.citations = document_details.citations.astype(int)\n",
    "document_details.rounds = document_details.rounds.astype(int)\n",
    "document_details.doc_id = document_details.doc_id.astype(int)\n",
    "document_details.author_id = document_details.author_id.astype(int)\n",
    "document_details = document_details.drop_duplicates()\n",
    "y = document_details.author_id.unique()\n",
    "author_profile = dict()\n",
    "for i in y:\n",
    "    author_profile[i] = []\n",
    "    a = document_details[document_details['author_id']==i]\n",
    "    b = min(a.year)\n",
    "    for j in list(range(b, 2015)):\n",
    "        c = a[a['year']==j]\n",
    "        d = c[c['accepted']==1]\n",
    "        e = c.shape[0]\n",
    "        if e==0:\n",
    "            author_profile[i].append(0)\n",
    "        else:\n",
    "            author_profile[i].append(d.shape[0]/float(e))\n",
    "\n",
    "high_accept = []\n",
    "low_accept = []\n",
    "medium_accept = []\n",
    "\n",
    "for author in author_profile:\n",
    "    d = author_profile[author]\n",
    "    e = 0.0\n",
    "    if len(d)==0:\n",
    "        continue\n",
    "    for i in d:\n",
    "        if i>.7:\n",
    "            e +=1\n",
    "    if e/len(d)>.7:\n",
    "        high_accept.append(author)\n",
    "        continue\n",
    "    e = 0.0\n",
    "    for i in d:\n",
    "        if i<.4:\n",
    "            e +=1\n",
    "    if e/len(d)>.8:\n",
    "        low_accept.append(author)\n",
    "        continue\n",
    "    medium_accept.append(author)\n",
    "\n",
    "out = open(\"./categories_new.txt\", \"w\")\n",
    "out1 = open(\"./high_acceptance.txt\", \"w\")\n",
    "out2 = open(\"./low_acceptance.txt\", \"w\")\n",
    "out3 = open(\"./medium_acceptance.txt\", \"w\")\n",
    "\n",
    "for author in high_accept:\n",
    "    out.write(str(author) + \",1\\n\")\n",
    "    out1.write(str(author) + \":\"+ \" \".join(str(x) for x in author_profile[author]) + \"\\n\")\n",
    "for author in low_accept:\n",
    "    out.write(str(author) + \",2\\n\")\n",
    "    out2.write(str(author) + \":\"+ \" \".join(str(x) for x in author_profile[author]) + \"\\n\")\n",
    "for author in medium_accept:\n",
    "    out.write(str(author) + \",3\\n\")\n",
    "    out3.write(str(author) + \":\"+ \" \".join(str(x) for x in author_profile[author]) + \"\\n\")\n",
    "\n",
    "out.close()\n",
    "out1.close()\n",
    "out2.close()\n",
    "out3.close()\n",
    "\n",
    "# Topic Diversity\n",
    "\n",
    "version = []\n",
    "keyword = []\n",
    "with open('versionkeywordsTable.txt') as infile:\n",
    "    for line in infile:\n",
    "        a = line.split()\n",
    "        version.append(a[0])\n",
    "        keyword.append(a[1])\n",
    "        \n",
    "d = {\n",
    "    'version_id':version,\n",
    "    'keyword':keyword\n",
    "}\n",
    "topic = pd.DataFrame(d)\n",
    "topic.keyword = topic.keyword.astype(int)\n",
    "topic.version_id = topic.version_id.astype(int)\n",
    "version = []\n",
    "auth = []\n",
    "with open('versionCod_DocCod.txt') as infile:\n",
    "    for line in infile:\n",
    "        a = line.split()\n",
    "        version.append(a[0])\n",
    "        auth.append(a[1])\n",
    "        \n",
    "d = {\n",
    "    'version_id':version,\n",
    "    'doc_id':auth\n",
    "}\n",
    "df = pd.DataFrame(d)\n",
    "df.doc_id = df.doc_id.astype(int)\n",
    "df.version_id = df.version_id.astype(int)\n",
    "df = pd.merge(topic,df)\n",
    "df = df[['doc_id','keyword']]\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "#High Accept\n",
    "topichigh = [[]]\n",
    "for i in high_accept:\n",
    "    l = document_details[document_details['author_id']==i]['doc_id'].unique()\n",
    "    c = []\n",
    "    for j in l:\n",
    "        c.extend(df[df['doc_id']==j]['keyword'].unique())\n",
    "    topichigh.append(c)\n",
    "    \n",
    "#Low Accept\n",
    "topiclow = []\n",
    "for i in low_accept:\n",
    "    l = document_details[document_details['author_id']==i]['doc_id'].unique()\n",
    "    c = []\n",
    "    for j in l:\n",
    "        c.extend(df[df['doc_id']==j]['keyword'].unique())\n",
    "    topiclow.append(c)\n",
    "    \n",
    "#Medium Accept\n",
    "topicmed = []\n",
    "for i in medium_accept:\n",
    "    l = document_details[document_details['author_id']==i]['doc_id'].unique()\n",
    "    c = []\n",
    "    for j in l:\n",
    "        c.extend(df[df['doc_id']==j]['keyword'].unique())\n",
    "    topicmed.append(c)\n",
    "    \n",
    "    \n",
    "# Topic Diversity\n",
    "numhigh = []\n",
    "for i in high_accept:\n",
    "    numhigh.append(document_details[document_details['author_id']==i].shape[0])\n",
    "\n",
    "d = {\n",
    "    'author_id':high_accept,\n",
    "    'topic':topichigh[1:],\n",
    "    'number':numhigh\n",
    "}\n",
    "df1 = pd.DataFrame(d)\n",
    "\n",
    "l = []\n",
    "for index,row in df1.iterrows():\n",
    "    l.append(len(set(row['topic']))/float(row['number']))\n",
    "    \n",
    "df1['Diversity'] = l\n",
    "\n",
    "numlow = []\n",
    "for i in low_accept:\n",
    "    numlow.append(document_details[document_details['author_id']==i].shape[0])\n",
    "\n",
    "d = {\n",
    "    'author_id':low_accept,\n",
    "    'topic':topiclow,\n",
    "    'number':numlow\n",
    "}\n",
    "df2 = pd.DataFrame(d)\n",
    "\n",
    "l = []\n",
    "for index,row in df2.iterrows():\n",
    "    l.append(len(set(row['topic']))/float(row['number']))\n",
    "    \n",
    "df2['Diversity'] = l\n",
    "\n",
    "nummed = []\n",
    "for i in medium_accept:\n",
    "    nummed.append(document_details[document_details['author_id']==i].shape[0])\n",
    "\n",
    "d = {\n",
    "    'author_id':medium_accept,\n",
    "    'topic':topicmed,\n",
    "    'number':nummed\n",
    "}\n",
    "df3 = pd.DataFrame(d)\n",
    "\n",
    "l = []\n",
    "for index,row in df3.iterrows():\n",
    "    l.append(len(set(row['topic']))/float(row['number']))\n",
    "    \n",
    "df3['Diversity'] = l\n",
    "\n",
    "df1 = df1[df1['number']>3]\n",
    "df2 = df2[df2['number']>3]\n",
    "df3 = df3[df3['number']>3]\n",
    "# High\n",
    "df1['Diversity'].mean()\n",
    "# Low\n",
    "df2['Diversity'].mean()\n",
    "# Medium\n",
    "df3['Diversity'].mean()\n",
    "\n",
    "\n",
    "# Final Dataset containing all features\n",
    "df = pd.read_csv('Final_dataset.csv')\n",
    "high_accept = []\n",
    "with open('high_acceptance.txt') as infile:\n",
    "    for line in infile:\n",
    "        a = line.split(':')\n",
    "        high_accept.append(int(a[0]))\n",
    "low_accept = []\n",
    "with open('low_acceptance.txt') as infile:\n",
    "    for line in infile:\n",
    "        a = line.split(':')\n",
    "        low_accept.append(int(a[0]))\n",
    "med_accept = []\n",
    "with open('medium_acceptance.txt') as infile:\n",
    "    for line in infile:\n",
    "        a = line.split(':')\n",
    "        med_accept.append(int(a[0]))\n",
    "        \n",
    "# Citations\n",
    "\n",
    "l1 = []\n",
    "for i in high_accept:\n",
    "    a = df[df['author_id']==i]['citations']\n",
    "    if len(a)==0 : continue\n",
    "    l1.extend(a)\n",
    "print sum(l1)/len(l1)\n",
    "\n",
    "\n",
    "l2 = []\n",
    "for i in low_accept:\n",
    "    a = df[df['author_id']==i]['citations']\n",
    "    if len(a)==0 : continue\n",
    "    l2.extend(a)\n",
    "print sum(l2)/len(l2)\n",
    "\n",
    "l3 = []\n",
    "for i in med_accept:\n",
    "    a = df[df['author_id']==i]['citations']\n",
    "    if len(a)==0 : continue\n",
    "    l3.extend(a)\n",
    "print sum(l3)/len(l3)\n",
    "\n",
    "print np.std(l1)\n",
    "print np.std(l2)\n",
    "print np.std(l3)\n",
    "\n",
    "# Closeness Centrality\n",
    "l4 = []\n",
    "for i in high_accept:\n",
    "    a = df[df['author_id']==i]['closeness_centrality']\n",
    "    if len(a)==0 : continue\n",
    "    l4.extend(a)\n",
    "np.mean(l4)\n",
    "\n",
    "l5 = []\n",
    "for i in low_accept:\n",
    "    a = df[df['author_id']==i]['closeness_centrality']\n",
    "    if len(a)==0 : continue\n",
    "    l5.extend(a)\n",
    "np.mean(l5)\n",
    "\n",
    "l6 = []\n",
    "for i in medium_accept:\n",
    "    a = df[df['author_id']==i]['closeness_centrality']\n",
    "    if len(a)==0 : continue\n",
    "    l6.extend(a)\n",
    "np.mean(l6)\n",
    "\n",
    "# Sentiment\n",
    "rl = pd.read_csv('Sentiment.csv')\n",
    "l4 = []\n",
    "for i in high_accept:\n",
    "    a = rl[rl['author_id']==i]['Len']\n",
    "    if len(a)==0 : continue\n",
    "    l4.extend(a)\n",
    "np.mean(l4)\n",
    "\n",
    "l4 = []\n",
    "for i in med_accept:\n",
    "    a = rl[rl['author_id']==i]['Len']\n",
    "    if len(a)==0 : continue\n",
    "    l4.extend(a)\n",
    "np.mean(l4)\n",
    "\n",
    "l4 = []\n",
    "for i in low_accept:\n",
    "    a = rl[rl['author_id']==i]['Len']\n",
    "    if len(a)==0 : continue\n",
    "    l4.extend(a)\n",
    "np.mean(l4)\n",
    "\n",
    "# Number of Rounds ( Accepted and Rejected)\n",
    "\n",
    "l7a = []\n",
    "l7r = []\n",
    "for i in high_accept:\n",
    "    a = df[df['author_id']==i]\n",
    "    b = a[a['accepted']==1]['rounds']\n",
    "    c = a[a['accepted']==0]['rounds']\n",
    "    if len(b)==0 :\n",
    "        pass\n",
    "    else:\n",
    "        l7a.extend(b)\n",
    "    if len(c)==0 :\n",
    "        pass\n",
    "    else:\n",
    "        l7r.extend(c)\n",
    "print np.mean(l7a)\n",
    "print np.mean(l7r)\n",
    "\n",
    "l8a = []\n",
    "l8r = []\n",
    "for i in low_accept:\n",
    "    a = df[df['author_id']==i]\n",
    "    b = a[a['accepted']==1]['rounds']\n",
    "    c = a[a['accepted']==0]['rounds']\n",
    "    if len(b)==0 :\n",
    "        pass\n",
    "    else:\n",
    "        l8a.extend(b)\n",
    "    if len(c)==0 :\n",
    "        pass\n",
    "    else:\n",
    "        l8r.extend(c)\n",
    "print np.mean(l8a)\n",
    "print np.mean(l8r)\n",
    "\n",
    "l9a = []\n",
    "l9r = []\n",
    "for i in med_accept:\n",
    "    a = df[df['author_id']==i]\n",
    "    b = a[a['accepted']==1]['rounds']\n",
    "    c = a[a['accepted']==0]['rounds']\n",
    "    if len(b)==0 :\n",
    "        pass\n",
    "    else:\n",
    "        l9a.extend(b)\n",
    "    if len(c)==0 :\n",
    "        pass\n",
    "    else:\n",
    "        l9r.extend(c)\n",
    "print np.mean(l9a)\n",
    "print np.mean(l9r)\n",
    "\n",
    "# Experience\n",
    "\n",
    "l10 = []\n",
    "for i in high_accept:\n",
    "    a = df[df['author_id']==i]\n",
    "    if a.shape[0]==0:\n",
    "        continue\n",
    "    l10.append(a.shape[0])\n",
    "np.mean(l10)\n",
    "\n",
    "l11 = []\n",
    "for i in low_accept:\n",
    "    a = df[df['author_id']==i]\n",
    "    if a.shape[0]==0:\n",
    "        continue\n",
    "    l11.append(a.shape[0])\n",
    "np.mean(l11)\n",
    "\n",
    "l12 = []\n",
    "for i in med_accept:\n",
    "    a = df[df['author_id']==i]\n",
    "    if a.shape[0]==0:\n",
    "        continue\n",
    "    l12.append(a.shape[0])\n",
    "np.mean(l12)\n",
    "\n",
    "from collections import Counter\n",
    "s = pd.Series(l10)\n",
    "s.value_counts()[:10].plot('bar')\n",
    "plt.xlabel('Experience')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('High Accept')\n",
    "plt.show()\n",
    "\n",
    "s = pd.Series(l11)\n",
    "s.value_counts()[:10].plot('bar')\n",
    "plt.xlabel('Experience')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Low Accept')\n",
    "plt.show()\n",
    "\n",
    "s = pd.Series(l12)\n",
    "s.value_counts()[:10].plot('bar')\n",
    "plt.xlabel('Experience')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Medium Accept')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
